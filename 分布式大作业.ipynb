{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分布式系统与大数据管理课程期末作业\n",
    "#建立SparkSession,SparkSession是读取数据、处理元数据、配置会话和管理集群资源的入口\n",
    "import os\n",
    "import sys\n",
    "os.environ['SPARK_HOME'] = \"/data/soft/spark/\"\n",
    "sys.path.append(\"/data/soft/spark//python\")\n",
    "sys.path.append(\"/data/soft/spark//python/lib/py4j-0.10.7-src.zip\")\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"finalProject\").getOrCreate()\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第一题\n",
    "#从HDFS读取数据集PM25city,存到2个分区\n",
    "dataFileRDD = sc.textFile(\"hdfs://192.168.142.131:9000/finalProject/PM25city\",2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['站号,经度,纬度,PM25,PM10,NO2,SO2,O3-1,O3-8h,CO,AQI,等级,year,month,day,hour,city',\n",
       " '99000,115.49,38.88,43,68,21,20,104,104,0.6,60,2,2018,8,1,0,北京',\n",
       " '99001,115.51,38.88,38,58,26,20,120,120,0.6,54,2,2018,8,1,0,北京']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFileRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of rows:398356\n",
      "Count of distinct rows:398356\n"
     ]
    }
   ],
   "source": [
    "#检查数据是否有重复的数据\n",
    "print('Count of rows:{0}'.format(dataFileRDD.count()))\n",
    "print('Count of distinct rows:{0}'.format(dataFileRDD.distinct().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of miss PM2.5:0\n",
      "Count of miss city:0\n"
     ]
    }
   ],
   "source": [
    "#检查城市和PM2.5字段的缺失情况\n",
    "print('Count of miss PM2.5:{0}'.format(dataFileRDD.filter(lambda row: row[3] == '').count()))\n",
    "print('Count of miss city:{0}'.format(dataFileRDD.filter(lambda row: row[16] == '').count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['站号', '经度', '纬度', 'PM25', 'PM10', 'NO2', 'SO2', 'O3-1', 'O3-8h',\n",
       "        'CO', 'AQI', '等级', 'year', 'month', 'day', 'hour', 'city'],\n",
       "       dtype='<U5'),\n",
       " array(['99000', '115.49', '38.88', '43', '68', '21', '20', '104', '104',\n",
       "        '0.6', '60', '2', '2018', '8', '1', '0', '北京'], dtype='<U6'),\n",
       " array(['99001', '115.51', '38.88', '38', '58', '26', '20', '120', '120',\n",
       "        '0.6', '54', '2', '2018', '8', '1', '0', '北京'], dtype='<U6')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#从初始RDD提取信息，提取后以numpy数组形式表示，并返回\n",
    "#在此引入re模块\n",
    "#纯python的效率低，因为Spark需要在python解释器和JVM之间连续切换\n",
    "def extractInformation(row):\n",
    "    import re\n",
    "    import numpy as np\n",
    "\n",
    "    selected_indices = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]\n",
    "    try:\n",
    "        rs = np.array(re.split(',',row))[selected_indices]\n",
    "    except:\n",
    "        rs = np.array(['-99'] * len(selected_indices))\n",
    "    return rs\n",
    "\n",
    "#map（）进行转换，挨个处理每行数据,使用take()而不是collect()来查看结果，collect()返回整个RDD，take()返回单个数据分区的前n行\n",
    "dataFileRDDConv = dataFileRDD.map(extractInformation)\n",
    "dataFileRDDConv.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of useful data PM2.5:398355\n",
      "Count of useful data City:398355\n"
     ]
    }
   ],
   "source": [
    "#查看是不是只有第一行是字段表达意思\n",
    "print('Count of useful data PM2.5:{0}'.format(dataFileRDDConv.filter(lambda row: row[3] != 'PM25').count()))\n",
    "print('Count of useful data City:{0}'.format(dataFileRDDConv.filter(lambda row: row[16] != 'city').count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#取出城市和PM25\n",
    "dataFileRDDConvPM25City = dataFileRDDConv.filter(lambda row : row[0] != '站号').map(lambda row: (row[16],float(row[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('北京', 43.0), ('北京', 38.0), ('北京', 50.0)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataFileRDDConvPM25City.count())\n",
    "dataFileRDDConvPM25City.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('天津', 589.0),\n",
       " ('济南', 534.0),\n",
       " ('成都', 359.0),\n",
       " ('呼和浩特', 530.0),\n",
       " ('昆明', 288.0),\n",
       " ('北京', 589.0),\n",
       " ('上海', 288.0),\n",
       " ('青岛', 425.0),\n",
       " ('厦门', 303.0),\n",
       " ('郑州', 521.0),\n",
       " ('乌鲁木齐', 546.0),\n",
       " ('海口', 167.0)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#求各城市的PM25最大值\n",
    "cityPM25Max = dataFileRDDConvPM25City.reduceByKey(max)\n",
    "cityPM25Max.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('天津', 0.0),\n",
       " ('济南', 0.0),\n",
       " ('成都', 0.0),\n",
       " ('呼和浩特', 0.0),\n",
       " ('昆明', 0.0),\n",
       " ('北京', 0.0),\n",
       " ('上海', 0.0),\n",
       " ('青岛', 0.0),\n",
       " ('厦门', 0.0),\n",
       " ('郑州', 0.0),\n",
       " ('乌鲁木齐', 0.0),\n",
       " ('海口', 0.0)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#求各城市的PM25最大值\n",
    "cityPM25Min = dataFileRDDConvPM25City.reduceByKey(min)\n",
    "cityPM25Min.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#求各城市的PM25最大值\n",
    "cityPM25Count = dataFileRDDConvPM25City.countByKey().items()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('北京', 37628), ('上海', 55441), ('天津', 55384), ('青岛', 39335), ('济南', 37609), ('厦门', 22304), ('郑州', 54302), ('乌鲁木齐', 14838), ('成都', 41165), ('呼和浩特', 13728), ('海口', 9398), ('昆明', 17223)])\n"
     ]
    }
   ],
   "source": [
    "print(cityPM25Count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cityPM25Average' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-dfce84178802>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcityPM25Min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataFileRDDConvPM25City\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcityPM25Average\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cityPM25Average' is not defined"
     ]
    }
   ],
   "source": [
    "cityPM25Min = dataFileRDDConvPM25City.groupByKey().mapValues(list).map(lambda x:(x[0],sum(x[1])/len(x[1])))\n",
    "cityPM25Average.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('天津', 56.37377220858009),\n",
       " ('济南', 83.21077401685767),\n",
       " ('成都', 56.21107737155351),\n",
       " ('呼和浩特', 41.75553613053613),\n",
       " ('昆明', 23.64524182778842),\n",
       " ('北京', 54.73764218135431),\n",
       " ('上海', 41.380602802979745),\n",
       " ('青岛', 60.41360111859667),\n",
       " ('厦门', 27.681850789096128),\n",
       " ('郑州', 94.13384405730912),\n",
       " ('乌鲁木齐', 83.67879768162825),\n",
       " ('海口', 16.023941264098745)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#求各城市PM25平均值\n",
    "cityPM25Average = dataFileRDDConvPM25City.groupByKey().mapValues(list).map(lambda x:(x[0],sum(x[1])/len(x[1])))\n",
    "cityPM25Average.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第二题 首先建立DataFrame \n",
    "#指定DataFrame的结构\n",
    "import pyspark.sql.types as typ\n",
    "labels = [\n",
    "    (\"站号\",typ.StringType()), (\"经度\",typ.StringType()), (\"纬度\",typ.StringType()),\n",
    "    (\"PM25\",typ.FloatType()),  (\"PM10\",typ.FloatType()),  (\"NO2\",typ.FloatType()),\n",
    "    (\"SO2\",typ.FloatType()),   (\"O3-1\",typ.FloatType()),  (\"O3-8h\",typ.FloatType()),\n",
    "    (\"CO\",typ.FloatType()),    (\"AQI\",typ.FloatType()),   (\"等级\",typ.FloatType()),\n",
    "    (\"year\",typ.StringType()), (\"month\",typ.StringType()),(\"day\",typ.StringType()),\n",
    "    (\"hour\",typ.StringType()), (\"city\",typ.StringType())]\n",
    "schema = typ.StructType([\n",
    "        typ.StructField(e[0], e[1], False) for e in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "398355"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#从hdfs文件生成DataFrame\n",
    "dataFileDF = spark.read.csv(\"hdfs://192.168.142.131:9000/finalProject/PM25city\",header=True,schema=schema,sep=',')\n",
    "dataFileDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+----+-----+----+----+-----+-----+---+----+---+----+-----+---+----+----+\n",
      "|   站号|    经度|   纬度|PM25| PM10| NO2| SO2| O3-1|O3-8h| CO| AQI| 等级|year|month|day|hour|city|\n",
      "+-----+------+-----+----+-----+----+----+-----+-----+---+----+---+----+-----+---+----+----+\n",
      "|99000|115.49|38.88|43.0| 68.0|21.0|20.0|104.0|104.0|0.6|60.0|2.0|2018|    8|  1|   0|  北京|\n",
      "|99001|115.51|38.88|38.0| 58.0|26.0|20.0|120.0|120.0|0.6|54.0|2.0|2018|    8|  1|   0|  北京|\n",
      "|99002|115.47|38.91|50.0| 72.0|22.0|17.0|113.0|113.0|0.7|69.0|2.0|2018|    8|  1|   0|  北京|\n",
      "|99004|115.45|38.88|52.0|108.0|46.0|21.0| 66.0| 66.0|0.7|79.0|2.0|2018|    8|  1|   0|  北京|\n",
      "|99006|116.36|39.87|55.0| 80.0|27.0| 2.0| 60.0| 60.0|1.3|75.0|2.0|2018|    8|  1|   0|  北京|\n",
      "+-----+------+-----+----+-----+----+----+-----+-----+---+----+---+----+-----+---+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataFileDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建临时视图以便适应spark SQL\n",
    "dataFileDF.createOrReplaceTempView(\"dataFileDFView\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+----+----+-----+---+----+-----+---+----+---+----+-----+---+----+----+\n",
      "|   站号|    经度|   纬度|PM25|PM10|  NO2|SO2|O3-1|O3-8h| CO| AQI| 等级|year|month|day|hour|city|\n",
      "+-----+------+-----+----+----+-----+---+----+-----+---+----+---+----+-----+---+----+----+\n",
      "|99052| 104.0|30.72|28.0|38.0| 72.0|4.0| 1.0|  1.0|1.2|40.0|1.0|2018|    8|  1|   0|  成都|\n",
      "|99053|104.19|30.67|41.0|79.0|120.0|8.0| 5.0|  5.0|1.2|65.0|2.0|2018|    8|  1|   0|  成都|\n",
      "|99054|104.08| 30.6|27.0|46.0| 87.0|2.0| 1.0|  1.0|0.7|46.0|1.0|2018|    8|  1|   0|  成都|\n",
      "|99055|104.12|30.63|37.0|50.0| 61.0|9.0|24.0| 24.0|1.1|53.0|2.0|2018|    8|  1|   0|  成都|\n",
      "|98537|104.06|30.66|56.0|89.0| 94.0|5.0|15.0| 15.0|1.2|77.0|2.0|2018|    8|  1|   0|  成都|\n",
      "+-----+------+-----+----+----+-----+---+----+-----+---+----+---+----+-----+---+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from dataFileDFView where city = '成都'\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 站号: string (nullable = true)\n",
      " |-- 经度: string (nullable = true)\n",
      " |-- 纬度: string (nullable = true)\n",
      " |-- PM25: float (nullable = true)\n",
      " |-- PM10: float (nullable = true)\n",
      " |-- NO2: float (nullable = true)\n",
      " |-- SO2: float (nullable = true)\n",
      " |-- O3-1: float (nullable = true)\n",
      " |-- O3-8h: float (nullable = true)\n",
      " |-- CO: float (nullable = true)\n",
      " |-- AQI: float (nullable = true)\n",
      " |-- 等级: float (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataFileDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     527|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#北京在2019.02.01这一天的数据量\n",
    "spark.sql(\"select count(*) from dataFileDFView where city = '北京' and year = '2019' and month = '2' and day = '1'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取北京201902数据，一共28天\n",
    "data_201902_beijing=spark.sql(\"select avg(AQI) as AQI,int(day) as day from dataFileDFView \\\n",
    "where year == '2019' and month == '2' and city == '北京' group by day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+\n",
      "|named_struct(year, year, month, month)|\n",
      "+--------------------------------------+\n",
      "|                             [2018, 8]|\n",
      "|                             [2019, 6]|\n",
      "|                             [2019, 2]|\n",
      "|                             [2019, 1]|\n",
      "+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#查询数据有多少月份\n",
    "data_month=spark.sql(\"select distinct (year,month) from dataFileDFView\")\n",
    "data_month.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.3 (default, Mar 27 2019, 22:11:17) \\n[GCC 7.3.0]'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_201902_beijing.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(AQI=86.70208728652752, day=1),\n",
       " Row(AQI=143.92638036809817, day=2),\n",
       " Row(AQI=121.0806142034549, day=3),\n",
       " Row(AQI=99.8956043956044, day=4),\n",
       " Row(AQI=170.7734082397004, day=5),\n",
       " Row(AQI=78.51948051948052, day=6),\n",
       " Row(AQI=37.265469061876246, day=7),\n",
       " Row(AQI=46.540117416829744, day=8),\n",
       " Row(AQI=31.506276150627617, day=9),\n",
       " Row(AQI=51.72357723577236, day=10),\n",
       " Row(AQI=83.689453125, day=11),\n",
       " Row(AQI=66.37869822485207, day=12),\n",
       " Row(AQI=51.111617312072894, day=13),\n",
       " Row(AQI=64.50769230769231, day=14),\n",
       " Row(AQI=37.24455205811138, day=15),\n",
       " Row(AQI=27.83622350674374, day=16),\n",
       " Row(AQI=35.91575091575091, day=17),\n",
       " Row(AQI=77.89781021897811, day=18),\n",
       " Row(AQI=156.68464730290455, day=19),\n",
       " Row(AQI=125.2179104477612, day=20),\n",
       " Row(AQI=169.34751773049646, day=21),\n",
       " Row(AQI=207.9163179916318, day=22),\n",
       " Row(AQI=195.65865384615384, day=23),\n",
       " Row(AQI=158.76923076923077, day=24),\n",
       " Row(AQI=75.21256038647343, day=25),\n",
       " Row(AQI=66.73849372384937, day=26),\n",
       " Row(AQI=101.7125, day=27),\n",
       " Row(AQI=142.02212389380531, day=28)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_201902_beijing.orderBy('day').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_201902_beijing.createOrReplaceTempView(\"data_201902_beijingView\")\n",
    "spark.sql(\"select * from data_201902_beijingView where AQI <= 50\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      11|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from data_201902_beijingView where AQI > 50 and AQI <= 100\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       5|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from data_201902_beijingView where AQI > 100 and AQI <=150\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       5|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from data_201902_beijingView where AQI > 151 and AQI <= 200\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       1|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from data_201902_beijingView where AQI > 200\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(AQI=87.12014134275618, day=1),\n",
       " Row(AQI=82.09510086455332, day=2),\n",
       " Row(AQI=99.23975409836065, day=3),\n",
       " Row(AQI=136.88491048593352, day=4),\n",
       " Row(AQI=70.6678486997636, day=5),\n",
       " Row(AQI=85.75257731958763, day=6),\n",
       " Row(AQI=54.767100977198695, day=7),\n",
       " Row(AQI=28.922365988909426, day=8),\n",
       " Row(AQI=31.957303370786516, day=9),\n",
       " Row(AQI=30.294117647058822, day=10),\n",
       " Row(AQI=44.08303249097473, day=11),\n",
       " Row(AQI=45.11092985318108, day=12),\n",
       " Row(AQI=44.024566473988436, day=13),\n",
       " Row(AQI=45.70854271356784, day=14),\n",
       " Row(AQI=28.83941605839416, day=15),\n",
       " Row(AQI=63.211717709720375, day=16),\n",
       " Row(AQI=41.034979423868315, day=17),\n",
       " Row(AQI=29.666011787819254, day=18),\n",
       " Row(AQI=42.13573407202216, day=19),\n",
       " Row(AQI=80.91749174917491, day=20),\n",
       " Row(AQI=85.51724137931035, day=21),\n",
       " Row(AQI=77.63254593175853, day=22),\n",
       " Row(AQI=161.39407744874714, day=23),\n",
       " Row(AQI=154.33130081300814, day=24),\n",
       " Row(AQI=116.38689217758986, day=25),\n",
       " Row(AQI=63.329351535836174, day=26),\n",
       " Row(AQI=44.53884711779449, day=27),\n",
       " Row(AQI=54.42175066312998, day=28)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取上海201902数据，一共28天\n",
    "data_201902_shanghai=spark.sql(\"select avg(AQI) as AQI,int(day) as day from dataFileDFView \\\n",
    "where year == '2019' and month == '2' and city == '上海' group by day\")\n",
    "data_201902_shanghai.orderBy('day').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_201902_shanghai.createOrReplaceTempView(\"data_201902_shanghaiView\")\n",
    "spark.sql(\"select * from data_201902_shanghaiView where AQI <= 50\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      12|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from data_201902_shanghaiView where AQI > 50 and AQI <= 100\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       2|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from data_201902_shanghaiView where AQI > 100 and AQI <= 150\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       2|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from data_201902_shanghaiView where AQI > 150 and AQI <= 200\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from data_201902_shanghaiView where AQI > 200 \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_201902_chengdu=spark.sql(\"select avg(AQI) as AQI,int(day) as day from dataFileDFView where year == '2019' and month == '2' and city == '成都' group by day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(AQI=85.16606498194946, day=1),\n",
       " Row(AQI=94.64737793851718, day=2),\n",
       " Row(AQI=72.72744360902256, day=3),\n",
       " Row(AQI=80.15724381625442, day=4),\n",
       " Row(AQI=149.9373913043478, day=5),\n",
       " Row(AQI=104.1522491349481, day=6),\n",
       " Row(AQI=85.05602716468591, day=7),\n",
       " Row(AQI=75.45640074211502, day=8),\n",
       " Row(AQI=56.70477815699659, day=9),\n",
       " Row(AQI=43.973333333333336, day=10),\n",
       " Row(AQI=53.89087656529517, day=11),\n",
       " Row(AQI=52.68352059925093, day=12),\n",
       " Row(AQI=74.76785714285714, day=13),\n",
       " Row(AQI=82.96625222024866, day=14),\n",
       " Row(AQI=70.23255813953489, day=15),\n",
       " Row(AQI=67.88945578231292, day=16),\n",
       " Row(AQI=55.329446064139944, day=17),\n",
       " Row(AQI=37.37623762376238, day=18),\n",
       " Row(AQI=57.574585635359114, day=19),\n",
       " Row(AQI=76.46460980036298, day=20),\n",
       " Row(AQI=73.71327433628319, day=21),\n",
       " Row(AQI=110.3632958801498, day=22),\n",
       " Row(AQI=117.37912087912088, day=23),\n",
       " Row(AQI=69.85191637630662, day=24),\n",
       " Row(AQI=73.10990990990992, day=25),\n",
       " Row(AQI=65.93320610687023, day=26),\n",
       " Row(AQI=58.45420560747664, day=27),\n",
       " Row(AQI=74.92293577981651, day=28)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_201902_chengdu.orderBy('day').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       2|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_201902_chengdu.createOrReplaceTempView(\"data_201902_chengduView\")\n",
    "spark.sql(\"select count(*) from data_201902_chengduView where AQI <= 50\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      22|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from data_201902_chengduView where AQI > 50 and AQI <= 100\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       4|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from data_201902_chengduView where AQI > 100 and AQI <= 150\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from data_201902_chengduView where AQI > 150 and AQI <= 200\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from data_201902_chengduView where AQI > 200 \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第三题 利用PySpark的ML包对成都的PM25进行回归分析\n",
    "dataFileDFChengDuRegPm25 =spark.sql(\"select PM25 as PM25,PM10 as PM10,NO2 as NO2,SO2 as SO2,CO as CO from dataFileDFView where city = '成都'\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41165"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFileDFChengDuRegPm25.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+----+---+\n",
      "|PM25|PM10|  NO2| SO2| CO|\n",
      "+----+----+-----+----+---+\n",
      "|28.0|38.0| 72.0| 4.0|1.2|\n",
      "|41.0|79.0|120.0| 8.0|1.2|\n",
      "|27.0|46.0| 87.0| 2.0|0.7|\n",
      "|37.0|50.0| 61.0| 9.0|1.1|\n",
      "|56.0|89.0| 94.0| 5.0|1.2|\n",
      "|16.0|25.0| 16.0| 8.0|0.9|\n",
      "|26.0|72.0| 65.0|14.0|1.0|\n",
      "|14.0|32.0| 53.0| 8.0|0.8|\n",
      "|28.0|54.0| 29.0| 5.0|0.6|\n",
      "| 5.0|14.0|  7.0| 9.0|0.4|\n",
      "+----+----+-----+----+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataFileDFChengDuRegPm25.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#排除离群值\n",
    "cols = ['PM25','PM10','NO2','SO2','CO']\n",
    "bounds = {}\n",
    "\n",
    "for col in cols:\n",
    "    quantiles = dataFileDFChengDuRegPm25.approxQuantile(col, [0.25, 0.75], 0.05)\n",
    "    IQR = quantiles[1] - quantiles[0]\n",
    "    bounds[col] = [quantiles[0] - 1.5 * IQR, quantiles[1] + 1.5 * IQR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PM25': [-21.5, 126.5],\n",
       " 'PM10': [-18.0, 174.0],\n",
       " 'NO2': [-25.5, 90.5],\n",
       " 'SO2': [0.5, 12.5],\n",
       " 'CO': [0.2499999701976776, 1.4500000178813934]}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PM25: float (nullable = true)\n",
      " |-- PM10: float (nullable = true)\n",
      " |-- NO2: float (nullable = true)\n",
      " |-- SO2: float (nullable = true)\n",
      " |-- CO: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataFileDFChengDuRegPm25.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "| corr(PM25, PM10)|\n",
      "+-----------------+\n",
      "|0.930331337950205|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#检查PM25与PM10的关系\n",
    "from pyspark.sql.functions import corr\n",
    "dataFileDFChengDuRegPm25.select(corr('PM25','PM10')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|   corr(PM25, NO2)|\n",
      "+------------------+\n",
      "|0.4764789366268503|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "dataFileDFChengDuRegPm25.select(corr('PM25','NO2')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|    corr(PM25, SO2)|\n",
      "+-------------------+\n",
      "|0.13638977091407592|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "dataFileDFChengDuRegPm25.select(corr('PM25','SO2')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|    corr(PM25, CO)|\n",
      "+------------------+\n",
      "|0.5110549464395842|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "dataFileDFChengDuRegPm25.select(corr('PM25','CO')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "vec = VectorAssembler(inputCols=['PM10','NO2','SO2','CO'],outputCol = 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = vec.transform(dataFileDFChengDuRegPm25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PM25: float (nullable = true)\n",
      " |-- PM10: float (nullable = true)\n",
      " |-- NO2: float (nullable = true)\n",
      " |-- SO2: float (nullable = true)\n",
      " |-- CO: float (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|            features|PM25|\n",
      "+--------------------+----+\n",
      "|[38.0,72.0,4.0,1....|28.0|\n",
      "|[79.0,120.0,8.0,1...|41.0|\n",
      "|[46.0,87.0,2.0,0....|27.0|\n",
      "|[50.0,61.0,9.0,1....|37.0|\n",
      "|[89.0,94.0,5.0,1....|56.0|\n",
      "+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_df = features_df.select('features','PM25')\n",
    "model_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41165"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28839, 2)\n"
     ]
    }
   ],
   "source": [
    "#划分数据集\n",
    "train_df,test_df = model_df.randomSplit([0.7,0.3])\n",
    "print((train_df.count(),len(train_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.7009, -0.0673, -0.5948, 6.9726])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lin_reg = LinearRegression(labelCol='PM25')\n",
    "lr_model = lin_reg.fit(train_df)\n",
    "lr_model.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.7874492318242212"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8743725638646"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#在测试数据上评估模型\n",
    "test_p = lr_model.evaluate(test_df)\n",
    "test_p.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129.7138600792581"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_p.meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
